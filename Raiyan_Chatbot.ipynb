{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Raiyan_Chatbot.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3cQh4e5k8Hc"
      },
      "source": [
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import pickle\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-exyv7yk_mG"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding, encoder_units, batch_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        self.enc_units = encoder_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding)\n",
        "        self.gru = tf.keras.layers.GRU(self.enc_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform', kernel_regularizer=tf.keras.regularizers.L2(0.001))\n",
        "    \n",
        "    def call(self, inputs, hidden_state):\n",
        "        embedded_inputs = self.embedding(inputs)\n",
        "        enc_outputs, thought_vector = self.gru(embedded_inputs, initial_state=hidden_state)\n",
        "        return enc_outputs, thought_vector"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwrcqB1Tk_pY"
      },
      "source": [
        "class Attention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(Attention, self).__init__()\n",
        "        \n",
        "        self.enc_output_layer = tf.keras.layers.Dense(units, kernel_regularizer=tf.keras.regularizers.L2(0.001))\n",
        "        self.thought_layer    = tf.keras.layers.Dense(units, kernel_regularizer=tf.keras.regularizers.L2(0.001))\n",
        "        self.final_layer      = tf.keras.layers.Dense(1    , kernel_regularizer=tf.keras.regularizers.L2(0.001))\n",
        "        \n",
        "    def call(self, enc_outputs, thought_vector):\n",
        "        thought_matrix = tf.expand_dims(thought_vector, 1)\n",
        "        \n",
        "        scores = self.final_layer(tf.keras.activations.tanh(self.enc_output_layer(enc_outputs) + self.thought_layer(thought_matrix)))\n",
        "        attention_weights = tf.keras.activations.softmax(scores, axis=-1)\n",
        "        \n",
        "        attention_output = attention_weights * enc_outputs \n",
        "        attention_output = tf.reduce_sum(attention_output, axis=1) \n",
        "        \n",
        "        return attention_output, attention_weights"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIWvMVtvk_st"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding, decoder_units, batch_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        self.dec_units = decoder_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding)\n",
        "        self.gru = tf.keras.layers.GRU(self.dec_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform', kernel_regularizer=tf.keras.regularizers.L2(0.001))\n",
        "        \n",
        "        self.attention = Attention(self.dec_units)\n",
        "        self.word_output = tf.keras.layers.Dense(vocab_size, kernel_regularizer=tf.keras.regularizers.L2(0.001))\n",
        "        \n",
        "    def call(self, inputs, enc_outputs, thought_vector):\n",
        "        attention_output, attention_weights = self.attention(enc_outputs, thought_vector)\n",
        "        \n",
        "        \n",
        "        embedded_inputs = self.embedding(inputs) \n",
        "        attention_output = tf.expand_dims(attention_output, 1) \n",
        "        concat_inputs = tf.concat([attention_output, embedded_inputs], axis=-1)\n",
        "        \n",
        "        decoder_outputs, hidden_state = self.gru(concat_inputs)\n",
        "        decoder_outputs = tf.reshape(decoder_outputs, (-1, decoder_outputs.shape[2])) \n",
        "        \n",
        "        final_outputs = self.word_output(decoder_outputs)\n",
        "        return final_outputs, hidden_state, attention_weights"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUI7C-xrk_4T"
      },
      "source": [
        "class Train:\n",
        "    def __init__(self):\n",
        "        self.optimizer = tf.keras.optimizers.Adam()\n",
        "        self.base_loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "        \n",
        "    def loss_function(self, y_real, y_pred):\n",
        "        base_mask = tf.math.logical_not(tf.math.equal(y_real, 0))\n",
        "        base_loss = self.base_loss_function(y_real, y_pred)\n",
        "        \n",
        "        mask = tf.cast(base_mask, dtype=base_loss.dtype)\n",
        "        final_loss = mask * base_loss\n",
        "        \n",
        "        return tf.reduce_mean(final_loss)\n",
        "    \n",
        "    def train_step(self, train_data, label_data, enc_hidden, encoder, decoder, batch_size, label_tokenizer):\n",
        "        loss = 0\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            enc_outputs, thought_vector = encoder(train_data, enc_hidden)\n",
        "            dec_hidden = thought_vector\n",
        "            dec_input = tf.expand_dims([label_tokenizer.word_index['<start>']] * batch_size, 1)\n",
        "            \n",
        "            for index in range(1, label_data.shape[1]):\n",
        "                outputs, dec_hidden, _ = decoder(dec_input, enc_outputs, dec_hidden)\n",
        "                \n",
        "                dec_input = tf.expand_dims(label_data[:, index], 1)\n",
        "                loss = loss + self.loss_function(label_data[:, index], outputs)\n",
        "        \n",
        "        word_loss = loss / int(label_data.shape[1])\n",
        "        \n",
        "        variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "        gradients = tape.gradient(loss, variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "        \n",
        "        return word_loss"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXoP0sZ_k_7V"
      },
      "source": [
        "class Data_Preprocessing:\n",
        "    def __init__(self):\n",
        "        self.temp = None\n",
        "    \n",
        "    def get_data(self, path):\n",
        "        file = open(path, 'r').read()\n",
        "        lists = [f.split('\\t') for f in file.split('\\n')]\n",
        "        \n",
        "        questions = [x[0] for x in lists]\n",
        "        answers = [x[1] for x in lists]\n",
        "        \n",
        "        return questions, answers\n",
        "    \n",
        "    def process_sentence(self, line):\n",
        "        line = line.lower().strip()\n",
        "        \n",
        "        line = re.sub(r\"([?!.,])\", r\" \\1 \", line)\n",
        "        line = re.sub(r'[\" \"]+', \" \", line)\n",
        "        line = re.sub(r\"[^a-zA-Z?!.,]+\", \" \", line)\n",
        "        line = line.strip()\n",
        "        \n",
        "        line = '<start> ' + line + ' <end>'\n",
        "        return line\n",
        "    \n",
        "    def word_to_vec(self, inputs):\n",
        "        tokenizer = Tokenizer(filters='')\n",
        "        tokenizer.fit_on_texts(inputs)\n",
        "        \n",
        "        vectors = tokenizer.texts_to_sequences(inputs)\n",
        "        vectors = pad_sequences(vectors, padding='post')\n",
        "        \n",
        "        return vectors, tokenizer"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcwMoMvuk_-z"
      },
      "source": [
        "data = Data_Preprocessing()\n",
        "\n",
        "questions, answers = data.get_data('/content/chatbot.txt')\n",
        "\n",
        "questions = [data.process_sentence(str(sentence)) for sentence in questions]\n",
        "answers = [data.process_sentence(str(sentence)) for sentence in answers]\n",
        "\n",
        "train_vectors, train_tokenizer = data.word_to_vec(questions)\n",
        "label_vectors, label_tokenizer = data.word_to_vec(answers)\n",
        "\n",
        "max_length_train = train_vectors.shape[1]\n",
        "max_length_label = label_vectors.shape[1]\n",
        "\n",
        "batch_size = 64\n",
        "buffer_size = train_vectors.shape[0]\n",
        "embedding_dim = 256\n",
        "steps_per_epoch = buffer_size//batch_size\n",
        "units = 1024"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdtMqzJolABR"
      },
      "source": [
        "vocab_train = len(train_tokenizer.word_index) + 1\n",
        "vocab_label = len(label_tokenizer.word_index) + 1"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-Eb-dkOlAEA"
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((train_vectors, label_vectors))\n",
        "dataset = dataset.shuffle(buffer_size)\n",
        "dataset = dataset.batch(batch_size, drop_remainder=True)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPHmhDRclAHB"
      },
      "source": [
        "encoder = Encoder(vocab_train, embedding_dim, units, batch_size)\n",
        "decoder = Decoder(vocab_label, embedding_dim, units, batch_size)\n",
        "trainer = Train()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPmZeG21lAJi",
        "outputId": "c9d746b5-cc09-402c-d44a-35dc247b666b"
      },
      "source": [
        "EPOCHS = 30\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    enc_hidden = tf.zeros((batch_size, units))\n",
        "    total_loss = 0\n",
        "    \n",
        "    for (batch_num, (train_data, label_data)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = trainer.train_step(train_data, label_data, enc_hidden, encoder, decoder, batch_size, label_tokenizer)\n",
        "        total_loss = total_loss + batch_loss\n",
        "        \n",
        "    print(f\"Epoch: {epoch}, Loss: {total_loss/steps_per_epoch}\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 1.5623279809951782\n",
            "Epoch: 2, Loss: 1.4190069437026978\n",
            "Epoch: 3, Loss: 1.3079081773757935\n",
            "Epoch: 4, Loss: 1.1954843997955322\n",
            "Epoch: 5, Loss: 1.0742905139923096\n",
            "Epoch: 6, Loss: 0.9466956853866577\n",
            "Epoch: 7, Loss: 0.8213114142417908\n",
            "Epoch: 8, Loss: 0.7093750238418579\n",
            "Epoch: 9, Loss: 0.6212669610977173\n",
            "Epoch: 10, Loss: 0.5410820245742798\n",
            "Epoch: 11, Loss: 0.47152963280677795\n",
            "Epoch: 12, Loss: 0.41393670439720154\n",
            "Epoch: 13, Loss: 0.35476791858673096\n",
            "Epoch: 14, Loss: 0.3045884668827057\n",
            "Epoch: 15, Loss: 0.25934770703315735\n",
            "Epoch: 16, Loss: 0.22261252999305725\n",
            "Epoch: 17, Loss: 0.18853700160980225\n",
            "Epoch: 18, Loss: 0.1634351760149002\n",
            "Epoch: 19, Loss: 0.14451169967651367\n",
            "Epoch: 20, Loss: 0.1290172040462494\n",
            "Epoch: 21, Loss: 0.1166825219988823\n",
            "Epoch: 22, Loss: 0.10803203284740448\n",
            "Epoch: 23, Loss: 0.10099603235721588\n",
            "Epoch: 24, Loss: 0.09551913291215897\n",
            "Epoch: 25, Loss: 0.08982820808887482\n",
            "Epoch: 26, Loss: 0.08712898939847946\n",
            "Epoch: 27, Loss: 0.08356238901615143\n",
            "Epoch: 28, Loss: 0.08219168335199356\n",
            "Epoch: 29, Loss: 0.08022579550743103\n",
            "Epoch: 30, Loss: 0.07896482199430466\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9uw-qH4Lszn",
        "outputId": "bdea521f-b6b2-479f-ca1a-9e1167c6d2e7"
      },
      "source": [
        "import pickle\n",
        "\n",
        "PKL_Filename =\"botencoder_pickle.pkl\"\n",
        "with open(PKL_Filename, 'wb') as file:\n",
        "  pickle.dump(encoder, file)\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: ram://4ec67983-7bb4-42dc-bfe3-803967ce50cf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: ram://4ec67983-7bb4-42dc-bfe3-803967ce50cf/assets\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f1056942d90> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wkbuIHKkS2A",
        "outputId": "850fafab-3b18-4e8d-d648-1b7946c102e2"
      },
      "source": [
        "import pickle\n",
        "\n",
        "PKL_Filename =\"botdecoder_pickle.pkl\"\n",
        "with open(PKL_Filename, 'wb') as file:\n",
        "  pickle.dump(decoder, file)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses, dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: ram://b479c6da-06b1-474a-8784-1f1fc055906f/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: ram://b479c6da-06b1-474a-8784-1f1fc055906f/assets\n",
            "WARNING:absl:<__main__.Attention object at 0x7f1056732ad0> has the same name 'Attention' as a built-in Keras object. Consider renaming <class '__main__.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f1056732190> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LEhRD48lAMV"
      },
      "source": [
        "class Chatbot:\n",
        "    def __init__(self, encoder, decoder, train_tokenizer, label_tokenizer, max_length_train, units):\n",
        "        self.train_tokenizer = train_tokenizer\n",
        "        self.label_tokenizer = label_tokenizer\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.units = units\n",
        "        self.data = Data_Preprocessing()\n",
        "        self.maxlen = max_length_train\n",
        "    \n",
        "    def clean_answer(self, answer):\n",
        "        answer = answer[:-1]\n",
        "        answer = ' '.join(answer)\n",
        "        return answer\n",
        "    \n",
        "    def predict(self, sentence):\n",
        "        sentence = self.data.process_sentence(sentence)\n",
        "        \n",
        "        sentence_mat = []\n",
        "        for word in sentence.split(\" \"):\n",
        "            try:\n",
        "                sentence_mat.append(self.train_tokenizer.word_index[word])\n",
        "            except:\n",
        "                return \"I Could not understand you, can you repeat again\"\n",
        "        \n",
        "        sentence_mat = pad_sequences([sentence_mat], maxlen=self.maxlen, padding='post')\n",
        "        sentence_mat = tf.convert_to_tensor(sentence_mat)\n",
        "        \n",
        "        enc_hidden = [tf.zeros((1, self.units))]\n",
        "        encoder_outputs, thought_vector = self.encoder(sentence_mat, enc_hidden)\n",
        "        \n",
        "        dec_hidden = thought_vector\n",
        "        dec_input = tf.expand_dims([label_tokenizer.word_index['<start>']], 0)\n",
        "        \n",
        "        answer = []\n",
        "        for i in range(1, self.maxlen):\n",
        "            pred, dec_hidden, _ = decoder(dec_input, encoder_outputs, dec_hidden)\n",
        "            \n",
        "            word = self.label_tokenizer.index_word[np.argmax(pred[0])]\n",
        "            answer.append(word)\n",
        "            \n",
        "            if word == '<end>':\n",
        "                return self.clean_answer(answer)\n",
        "            \n",
        "            dec_input = tf.expand_dims([np.argmax(pred[0])], 0)\n",
        "        \n",
        "        return self.clean_answer(answer)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJh1kebSlAPB"
      },
      "source": [
        "bot = Chatbot(encoder, decoder, train_tokenizer, label_tokenizer, max_length_train, units)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_T_FUQ-LlzLF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ffa10c7-3f7c-4702-8121-8b0c9409b78b"
      },
      "source": [
        "question = ''\n",
        "while True:\n",
        "    question = str(input('You:'))\n",
        "    if question == 'quit' or question == 'Quit':\n",
        "        break\n",
        "        \n",
        "    answer = bot.predict(question)\n",
        "    print(f'Bot: {answer}')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You:hello\n",
            "Bot: hello\n",
            "You:good Morning\n",
            "Bot: morning\n",
            "You:good evening\n",
            "Bot: good evening\n",
            "You:bye\n",
            "Bot: bye\n",
            "You:do you like to chat\n",
            "Bot: I Could not understand you, can you repeat again\n",
            "You:i love you\n",
            "Bot: i love you .\n",
            "You:welcome\n",
            "Bot: what do you re done .\n",
            "You:what did you do?\n",
            "Bot: i watered all the plants .\n",
            "You:where is your house\n",
            "Bot: it s in a house .\n",
            "You:bye bye robot\n",
            "Bot: I Could not understand you, can you repeat again\n",
            "You:quit\n"
          ]
        }
      ]
    }
  ]
}